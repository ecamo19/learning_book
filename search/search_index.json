{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Notes","text":"<p>This site contains notes, chapters and tutorials read in the internet.  </p>"},{"location":"code_archive/programming_concepts/hash_maps/","title":"Hash maps","text":"<p>Tabular data there is a relationship between the elements of a row. Each column corresponds to a different feature of the row. For example:</p> <p>Musician        |  State of Birth Miles Davis     |  Illinois John Coltrane   |  North Carolina Duke Ellington  |  Ohio Dizzy Gillespie |  South Carolina Thelonious Monk |  North Carolina</p> <p>This kind of table, with only two columns, represents a special relationship that mathematicians would call a \u201cmap\u201d. This table maps states to state flowers, but many other relationships can be modeled with maps.</p> <p>When talking about a map we describe the inputs (jazz musicians, in this case) as the keys to the map. The output (here the state of origin) is said to be the value at a given key.</p> <p>In order for a relationship to be a map, every key that is used can only be the key to a single value.</p> <p>For creating a hash map we use arrays bescause with these we can use the index for keep track of values in memory (index ==  key).</p> <p>A hashing function is a function that tranform a string (key) into a index.</p> <p>The most important aspect is that a hashing function returns an array index as output.</p> <p>The Hash map implementation needs to know the size of our array.</p> <p>In order for our hash map implementation to guarantee that it returns an index that fits into the underlying array, the hash function will first compute a value using some scoring metric: this is the hash value, hash code, or just the hash. Our hash map implementation then takes that hash value mod the size of the array. This guarantees that the value returned by the hash function can be used as an index into the array we\u2019re using.</p> <p>Key         =&gt;  gar</p> <p>Hashed into =&gt;  [\"g\", \"a\", \"r\"]</p> <p>code_point  =&gt;  [\"103\", \"97\", \"114\"]</p> <p>Add them up =&gt;  103 + 97 + 114</p> <p>Hash Value  =&gt;  314</p> <p>The storage location at the index given by a hash is called the hash bucket.</p> <p>Collisions</p> <p>Remember hash functions are designed to compress data from a large number of possible keys to a much smaller range. Because of this compression, it\u2019s likely that our hash function might produce the same hash for two different keys. This is known as a hash collision. There are several strategies for resolving hash collisions.</p> <p>There are different hash collision strategies. Two important ones are separate chaining, where each array index points to a different data structure, and open addressing, where a collision triggers a probing sequence to find where to store the value for a given key.</p> <p>Hash map: A key-value store that uses an array and a hashing function to save and retrieve values. Key: The identifier given to a value for later retrieval. Hash function: A function that takes some input and returns a number. Compression function: A function that transforms its inputs into some smaller range of possible outputs.</p> <p>Recipe for saving to a hash table:</p> <ul> <li>Take the key and plug it into the hash function, getting the hash code.</li> <li>Modulo that hash code by the length of the underlying array, getting an array index.</li> <li>Check if the array at that index is empty, if so, save the value (and the key) there.</li> <li>If the array is full at that index continue to the next possible position depending on your collision strategy.</li> </ul> <p>Recipe for retrieving from a hash table:</p> <ul> <li>Take the key and plug it into the hash function, getting the hash code.</li> <li>Modulo that hash code by the length of the underlying array, getting an array index.</li> <li>Check if the array at that index has contents, if so, check the key saved there.</li> <li>If the key matches the one you're looking for, return the value.</li> <li>If the keys don't match, continue to the next position depending on your collision strategy.</li> </ul>"},{"location":"code_archive/programming_concepts/hash_maps/#hash-function-example","title":"Hash Function example\u00b6","text":""},{"location":"how_to/how_to_cluster_in_python/","title":"How to cluster in python","text":"<p>https://programminghistorian.org/en/lessons/clustering-with-scikit-learn-in-python</p>"},{"location":"how_to/how_to_set_virtual_env_in_python/","title":"How to set a virtual enviroment for Python","text":"<pre><code>knitr::opts_chunk$set(echo = TRUE, comment = \"\", fig.align = 'center',\n                      fig.width = 16, fig.height = 10, eval = F)\n</code></pre> <p>All the code was gather from this video</p>"},{"location":"how_to/how_to_set_virtual_env_in_python/#check-pip-version","title":"Check pip version","text":"<pre><code>pip --version\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#install-pipenv-at-the-user-level","title":"Install pipenv at the user level","text":"<pre><code>pip install --user pipenv\n</code></pre> <p>If you see a warning saying: ' bla bla bla which is not in path ' you have to edit the bash profile</p> <pre><code># Open bash_profile\nnano ~/.bash_profile\n\n# Add to the profile \nexport PATH=\"add path that appears in the error:$PATH\"\n\nrestart terminal\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#check-that-pipenv-is-install","title":"Check that pipenv is install","text":"<pre><code>pipenv --version\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#set-up-virtual-enviroment-for-the-first-time","title":"Set-up virtual enviroment for the first time","text":""},{"location":"how_to/how_to_set_virtual_env_in_python/#set-up-last-python-version","title":"Set up last python version","text":"<pre><code># Initialize python virtual env\n# Three means the python version, if you work with python 2 change to to two  \n# Run this in the folder the you are working \n# Should create a file called Pipfile\n\npipenv --three\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#set-up-an-old-python-version","title":"Set up an old python version","text":"<pre><code># Check all the python versions available in my computer\nls /usr/bin/python*\n\n# Install the version wanted\npipenv install --python 3.7 \n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#daily-workflow","title":"Daily workflow","text":"<ul> <li> <p>The steps from above are done to set up the pipenv for the first time.</p> </li> <li> <p>The following steps are done every single time I work in the project</p> </li> </ul>"},{"location":"how_to/how_to_set_virtual_env_in_python/#open-a-shell-terminal-inside-a-virtual-environment","title":"Open a shell (terminal) inside a virtual environment","text":"<pre><code>pipenv shell\n\n# Exit the shell\nexit\n</code></pre> <pre><code># Open python inside the virtual environment\npython3\n\n# Check package version\nprint(numpy.__version__)\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#install-packages-in-pipenv","title":"Install packages in pipenv","text":"<ul> <li> <p>Remember to run this command inside the pipenv shell </p> </li> <li> <p>Make sure that the package appears in the PIPfile</p> </li> </ul> <pre><code># This install any version of numpy aka the latest version\npipenv install _package_\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#install-a-specific-version-of-the-package-in-pipenv","title":"Install a specific version of the package in pipenv","text":"<pre><code># This install a specific version of a package \npipenv install _package_ == 2.18.1 \n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#create-requirementtxt-file","title":"Create requirement.txt file","text":"<pre><code>pipenv lock -r &gt; requirements.txt\n</code></pre>"},{"location":"how_to/how_to_set_virtual_env_in_python/#removing-a-virtualenv","title":"Removing a virtualenv","text":"<pre><code>test_project pipenv --rm\n</code></pre>"},{"location":"linux/docker/concepts_docker/","title":"Docker","text":""},{"location":"linux/docker/concepts_docker/#_1","title":"Docker","text":""},{"location":"linux/docker/concepts_docker/#project-layout","title":"Project layout","text":""},{"location":"linux/terminal_command_line/obtaining_data/","title":"Obtaining data","text":"<p>Files can be transferred in and out of the Docker container.</p> In\u00a0[\u00a0]: <pre>#docker ps to view listing which includes container_id\ndocker cp foo.txt container\n</pre> In\u00a0[\u00a0]: <pre># Download a web page \ncurl \"https://en.wikipedia.org/wiki/Costa_Rica\" | trim\n\n# Save, -s == silence progress meter\ncurl -s \"https://en.wikipedia.org/wiki/Costa_Rica\" &gt; costarica.html\n</pre> In\u00a0[\u00a0]: <pre>\n</pre> In\u00a0[\u00a0]: <pre>\n</pre> In\u00a0[\u00a0]: <pre>\n</pre> In\u00a0[\u00a0]: <pre>\n</pre> In\u00a0[\u00a0]: <pre>\n</pre> In\u00a0[\u00a0]: <pre>\n</pre>"},{"location":"linux/terminal_command_line/obtaining_data/#obtaining-data","title":"Obtaining data\u00b6","text":""},{"location":"linux/terminal_command_line/obtaining_data/#copy-local-files-to-the-docker-image","title":"Copy local files to the Docker image\u00b6","text":""},{"location":"linux/terminal_command_line/obtaining_data/#curl","title":"Curl\u00b6","text":""},{"location":"stats_and_modeling/applications/law_of_large_numbers/","title":"Law of large numbers","text":"In\u00a0[1]: <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n</pre> In\u00a0[2]: <pre>def coin_flip_experiment():\n  # defining our two coins as lists\n  coin1 = ['Heads', 'Tails']\n  coin2 = ['Heads', 'Tails']\n \n  # \"flipping\" both coins randomly\n  coin1_result = np.random.choice(coin1)\n  coin2_result = np.random.choice(coin2)\n \n  # checking if both flips are heads\n  if coin1_result == 'Heads' and coin2_result == 'Heads':\n    return 1\n  else:\n    return 0\n</pre> In\u00a0[13]: <pre>#how many times we run the experiment\n\nnum_trials = 2\n\n# Empty list\nprop = []\nflips = []\n\n# keep track of the number of times heads pops up twice\ntwo_heads_counter = 0\n \n# perform the experiment five times\nfor flip in range(num_trials):\n    \n  # if both coins are heads add 1 to the counter\n  two_heads_counter += coin_flip_experiment()\n  \n  # keep track of the proportion of two heads at each flip \n  prop.append(two_heads_counter/(flip + 1))\n  \n  # keep a list for number of flips\n  flips.append(flip + 1)\n \n# plot all flips and proportion of two heads\n\nplt.plot(flips, prop, label = 'Experimental Probability')\nplt.xlabel('Number of Flips')\nplt.ylabel('Proportion of Two Heads')\n\nplt.hlines(0.25, 0, num_trials, colors = 'orange', label = 'True Probability')\nplt.legend()\n\n\n \nplt.show()\n</pre>"},{"location":"stats_and_modeling/theory/bias_variance_tradeoff/","title":"Bias variance tradeoff","text":""},{"location":"stats_and_modeling/theory/bias_variance_tradeoff/#bias-variance-trade-off","title":"Bias-Variance trade-off\u00b6","text":"<p>In a perfect scenario, a statistical method should achieve low variance and low bias</p> <ul> <li>Bias </li> </ul> <p>Bias is defined as the error introduced by approximating a real life problem (which in nature could have complicated relationships) by a simple model</p> <p>I associate bias as a model simplification. Like Fitting a linear model that follow a non-lineal trend</p> <ul> <li>Variance</li> </ul> <p>How much a function $f$ would change if we estimated it using a different train set?  High variance indicate that if we change a data point in the data the function $f$ will change.</p> <p>I associate variance with overfit. Models with high variance have high flexibility because those model adjust following each data point.</p> <p>For example linear models have low variance but they could have high bias</p> <p>Highly correlated predictors can lead to collinearity issues and this can greatly increase the model variance.</p> <p>More complex models can have very high variance, which leads to over-\ufb01tting. On the other hand, simple models tend not to over-\ufb01t, but under-\ufb01t if they are not \ufb02exible enough to model the true relationship (thus high bias).</p>"},{"location":"stats_and_modeling/theory/dimesion_reduction/","title":"Dimesion reduction","text":""},{"location":"stats_and_modeling/theory/dimesion_reduction/#dimension-reduction-methods","title":"Dimension Reduction Methods\u00b6","text":"<p>When n &lt;&lt; p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this.</p> <p>These methods control variance by transforming the predictors and then fit a least squares model using these transformed variables. With these methods overfitting is avoid by using a less flexible approach.</p> <p>If the correlation among predictors is high, then the ordinary least squares solution for multiple linear regression will have high variability and will become unstable OR  if n &lt;&lt; p  least squares will be unable to \ufb01nd a unique set of regression coe\ufb03cients that minimize the SSE</p>"},{"location":"stats_and_modeling/theory/dimesion_reduction/#pcr","title":"PCR\u00b6","text":"<p>Does not perform feature selection</p> <p>Sometimes dimension reduction via PCA does not necessarily produce new predictors that explain the response.</p> <ul> <li>The scores indicates if a value is below or above average. For example if the score &lt; 0 in a PCA with two variables, then this indicates a city with below-average population size and below average ad spending. </li> </ul>"},{"location":"stats_and_modeling/theory/dimesion_reduction/#pls","title":"PLS\u00b6","text":"<p>Kuhn: We recommend using PLS when there are correlated predictors and a linear regression-type solution is desired</p> <p>PLS identifies new features in a supervised way by making use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.</p> <p>PLS \ufb01nds linear combinations of the predictors</p>"},{"location":"stats_and_modeling/theory/dimesion_reduction/#what-goes-wrong-in-high-dimensions","title":"What Goes Wrong in High Dimensions?\u00b6","text":"<p>Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating $\\sigma^2$ is problematic.</p>"},{"location":"stats_and_modeling/theory/dimesion_reduction/#pcr-vrs-pls","title":"PCR vrs PLS\u00b6","text":"<p>Since the directions in PLS are obtained by integrating the response variable, they explain more variation in Y than the principal components regression do. PLS also gives similar loadings to correlated variables, but variables that are correlated with the Y receive more weights compared with PCR.</p>"},{"location":"stats_and_modeling/theory/model_selection/","title":"Model selection","text":""},{"location":"stats_and_modeling/theory/model_selection/#model-selection","title":"Model selection\u00b6","text":""},{"location":"stats_and_modeling/theory/model_selection/#best-subset-selection","title":"Best Subset Selection\u00b6","text":"<p>Fit a separate least squares regression for each possible combination of $p$ predictors. That is, fit all possible combinations and evaluate all the possibilities.</p> <p>The problem with this is that we have to evaluate $2^p$ models which is computationally expensive.</p>"},{"location":"stats_and_modeling/theory/model_selection/#forward-stepwise-selection","title":"Forward Stepwise Selection\u00b6","text":"<p>This method starts with a null model with no predictors, then add a predictor, evaluate the model and repeats.</p> <p>Forward selection can be applied when n &lt; p but only to $M_0$ ..... $M_{n - 1}$  since  n &gt;= p will not yield a unique solution.</p>"},{"location":"stats_and_modeling/theory/model_selection/#backward-stepwise-selection","title":"Backward Stepwise Selection\u00b6","text":"<p>This method begins with the full least squares model containing all p predictors. This works when n &gt; p</p>"},{"location":"stats_and_modeling/theory/model_selection/#choosing-the-optimal-model-cp-aic-bic-and-adjusted-r5e2","title":"Choosing the Optimal Model Cp , AIC, BIC, and Adjusted R^2\u00b6","text":"<ul> <li><p>Cp: Add a 2d$\\sigma^2$ penalty that increases as the number of predictor increase. Cp ~ AIC</p> </li> <li><p>BIC: Usually BIC chooses a model with less predictors that the one chosen by Cp</p> </li> </ul>"},{"location":"stats_and_modeling/theory/model_selection/#one-standard-error-rule","title":"One-standard-error rule.\u00b6","text":"<p>We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</p>"},{"location":"stats_and_modeling/theory/model_selection/#do-not-use-r5e2-or-rss-for-selecting-a-model","title":"Do not use R^2 or RSS for selecting a model\u00b6","text":"<p>The training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R^2  cannot be used to select from among a set of models with di\ufb00erent numbers of variables. $R^2$ increases with the number of predictors while RSS decrease.</p>"},{"location":"stats_and_modeling/theory/multiple_regression/","title":"Multiple regression","text":""},{"location":"stats_and_modeling/theory/multiple_regression/#multiple-regression","title":"Multiple Regression\u00b6","text":"<p>The interpretability of coe\ufb03cients makes it very attractive as a modeling tool. At the same time, the characteristics that make it interpretable also make it prone to potentially fatal \ufb02aws.</p> <p>A unique inverse of this matrix exists when (1) no predictor can be determined from a combination of one or more of the other predictors and (2) the number of samples is greater than the number of predictors. If the data fall under either of these conditions, then a unique set of regression co- e\ufb03cients does not exist.</p> <p>To diagnose multicollinearity in the context of linear regression, the variance in\ufb02ation factor can be used.</p> <p>Another drawback of multiple linear regression is that its solution is linear in the parameters. One visual clue to understanding if the relationship between predictors and the response is not linear is to examine the basic diagnostic plots. Curvature in the predicted-versus-residual plot is a primary indicator that the underlying relationship is not linear.</p> <p>A third notable problem with multiple linear regression is that it is prone to chasing observations that are away from the overall trend of the majority of the data.Recall that linear regression seeks to \ufb01nd the parameter estimates that minimize SSE; hence, observations that are far from the trend of the majority of the data will have exponentially large residuals.</p> <p>There are no tuning parameters for multiple linear regression. This fact, however, does not impugn the practitioner from using rigorous model validation tools, especially when using this model for prediction. In fact, we must use the same training and validation techniques described in Chap. 4 to understand the predictive ability of this model on data which the model has not seen.</p>"},{"location":"stats_and_modeling/theory/probability/","title":"Probability","text":""},{"location":"stats_and_modeling/theory/probability/#probability","title":"Probability\u00b6","text":"<p>ref</p>"},{"location":"stats_and_modeling/theory/probability/#set-theory","title":"Set theory\u00b6","text":"<p>Set is a collection of things. For example, we can use a set to represent items in a backpack. An example of a set could be: $${Book,Paper,Folder,Hat,Pen,Snack}$$</p> <p>Sets also follow two key rules:</p> <ul> <li>Each element in a set is distinct.</li> <li>The elements in a set are in no particular order.</li> </ul> <p>Sets can also contain subsets. Set A is a subset of set B if all the elements in A exist within B. For example:</p> $$A = {1,2,3}$$$$B = {1,2,3,4,5}$$"},{"location":"stats_and_modeling/theory/probability/#experiments-and-sample-spaces","title":"Experiments and Sample Spaces\u00b6","text":"<p>In probability, an experiment is something that produces observation(s) with some level of uncertainty.</p> <p>A sample point is a single possible outcome of an experiment.</p> <p>Finally, a sample space is the set of all possible sample points for an experiment.</p> <p>For example, suppose that we run an experiment where we flip a coin twice and record whether each flip results in heads or tails. There are four sample points in this experiment: two heads (HH), tails and then heads (TH), heads and then tails (HT), or two tails (TT). We can write the full sample space for this experiment as follows:</p> $$S = \\{HH, TT, HT, TH\\}$$<p>Suppose we are interested in the probability of one specific outcome: HH. A specific outcome (or set of outcomes) is known as an event and is a subset of the sample space. Three events we might look at in this sample space are:</p> $$Getting\\ Two\\ Heads$$$$A={HH}$$$$Getting\\ Two\\ Tails$$$$B={TT}$$$$Getting\\ Both\\ a\\ Heads\\ and\\ Tails$$$$C={HT,TH}$$"},{"location":"stats_and_modeling/theory/probability/#the-frequentist-definition-of-probability","title":"The frequentist definition of probability\u00b6","text":"<p>If we run an experiment an infinite amount of times, the probability of each event is the proportion of times it occurs. Unfortunately, we don\u2019t have the ability to flip two coins an infinite amount of times \u2014 but we can estimate probabilities by choosing some other large number, such as 1000. For example:</p> $${HH}: 252$$$${TT}: 247$$$${HT}: 256$$$${TH}: 245$$<p>To calculate the estimated probability of any one outcome, we use the following formula:</p> $$P(event) = \\frac{Number\\ of\\ times\\ the\\ event\\ occurred}{Total\\ number\\ of\\ trials}$$<p>In this scenario, a trial is a single run of our experiment (two coin flips). So, the probability of two heads on two coin flips is approximately:</p> $$P(HH) = \\frac{255}{1000}$$"},{"location":"stats_and_modeling/theory/probability/#law-of-large-numbers","title":"Law of Large Numbers\u00b6","text":"<p>We can\u2019t repeat our random experiment an infinite amount of times. However, we can still flip both coins a large number of times. As we flip both coins more and more, the observed proportion of times each event occurs will converge to its true probability. This is called the law of large numbers.</p> <p>Check the applications folder</p>"},{"location":"stats_and_modeling/theory/probability/#rules-of-probability","title":"Rules of probability\u00b6","text":""},{"location":"stats_and_modeling/theory/regularization/","title":"Regularization","text":""},{"location":"stats_and_modeling/theory/regularization/#penalized-models","title":"Penalized Models\u00b6","text":"<p>When n &lt;&lt; p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this.</p> <p>Penalized methods work by controlling variance (less flexible methods) by shrinking coefs towards zero or exactly zero. With these methods overfitting is avoid by using a less flexible approach.</p> <p>By sacri\ufb01cing some bias, we can often reduce the variance enough to make the overall MSE lower than unbiased models.</p>"},{"location":"stats_and_modeling/theory/regularization/#ridge-regression","title":"Ridge Regression\u00b6","text":"<p>Ridge Regression will include all p predictors in the final model. This method just shrink coefs towards zero.</p> <p>Ridge regression (Hoerl 1970) adds a penalty on the sum of the squared regression parameters (l2)</p> <p>$\\lambda$ = Penalty term (l2), the larger the $\\lambda$ is the smallest (towards zero) the coefs are.</p> <p>Ridge regression\u2019s advantage over least squares is rooted in the bias-variance trade-o\ufb00. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. At some point MSE will decrease.</p> <p>Ridge regression works best in situations where the least squares estimates have high variance.</p>"},{"location":"stats_and_modeling/theory/regularization/#lasso-regression","title":"Lasso Regression\u00b6","text":"<p>The lasso shrinks the coefs to zero which can be used for variable selection</p> <p>Differences between lasso and ridge</p> <ul> <li>Penalty term</li> <li>Lasso performs feature selection</li> <li>Lasso will perform better in a scenario where a few p have a high coefs and the others are close to zero</li> <li>Ridge will perform better when there are many predictors with with coefs of the same  ~size</li> </ul>"},{"location":"stats_and_modeling/theory/resampling_methods/","title":"Resampling methods","text":""},{"location":"stats_and_modeling/theory/resampling_methods/#resampling-methods","title":"Resampling Methods\u00b6","text":"<p>Resampling methods are used in the absence of very large designated test sets that can be used to directly estimate test error rates</p> <ul> <li>Validation Set Approach </li> </ul> <p>Good when n is large. This method consist in dividing randomly the data set into Training and Test (Validation) data sets</p> <p>Drawbacks:</p> <p>1) High Variable, depends on the observations chosen  2) Not all the data is included in training the model</p> <ul> <li>Leave-one-out cross validation (LOOCV)</li> </ul> <p>In this method a single pair observations ($x_1$,$y_1$) are left out for validation and the rest of the data is used for training being repeated n times</p> <p>Drawbacks:</p> <p>1) Expensive to implement (time consuming, the validation is done in n observations)</p> <ul> <li>k-fold Cross Validation</li> </ul> <p>Randomly divide the whole data set in k groups of equal size. The first fold is used as validation set and the others are used as training sets. This procedure is repeated k times, each time, a different fold is treated as validation test.</p> <p>For example if k = 10 then the data set is divided into 10 folds, the first one will be used as validation set and then the statistical method (i.e. linear model) is fitted on each one of the other 9 folds. This is repeated until each one of the folds has being used as validation sets.</p>"}]}